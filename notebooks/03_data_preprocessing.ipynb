{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw stock data\n",
    "stock_data = pd.read_csv('../data/raw/AAPL_stock_data.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: Using 'Close' price for prediction\n",
    "data = stock_data[['Close']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>43.064999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>43.057499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>43.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>43.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>43.587502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Close\n",
       "Date                 \n",
       "2018-01-02  43.064999\n",
       "2018-01-03  43.057499\n",
       "2018-01-04  43.257500\n",
       "2018-01-05  43.750000\n",
       "2018-01-08  43.587502"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization (Min-Max scaling)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating sequences for time series\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        sequences.append(data[i-seq_length:i])\n",
    "        labels.append(data[i])\n",
    "    return np.array(sequences), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 50    # Considering the last 50 days to predict the next day's price\n",
    "X, y = create_sequences(scaled_data, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (846, 50, 1), Validation data: (181, 50, 1), Test data: (182, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training, validation, and test sets\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "test_size = len(X) - train_size - val_size\n",
    "\n",
    "X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]\n",
    "y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]\n",
    "\n",
    "# Summary of splits\n",
    "print(f\"Training data: {X_train.shape}, Validation data: {X_val.shape}, Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/scaler.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the MinMaxScaler as a serialized object\n",
    "scaler_path = \"../data/processed/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaled data (for future predictions) and training, validation, test sets\n",
    "np.save(\"../data/processed/scaled_data.npy\", scaled_data)\n",
    "np.save(\"../data/processed/X_train.npy\", X_train)\n",
    "np.save(\"../data/processed/y_train.npy\", y_train)\n",
    "np.save(\"../data/processed/X_val.npy\", X_val)\n",
    "np.save(\"../data/processed/y_val.npy\", y_val)\n",
    "np.save(\"../data/processed/X_test.npy\", X_test)\n",
    "np.save(\"../data/processed/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Summary\n",
    "In this phase, the stock price data was successfully prepared for a time series modeling. Below are the completed key steps:\n",
    "\n",
    "1. **Feature Selection:**\n",
    "    - The `Close` price was selected as the primary feature for predicting future stock prices, based on its importance in financial analysis.\n",
    "\n",
    "2. **Data Normalization:**\n",
    "    - Using `MinMaxScaler, the data was normalized to a range between 0 and 1 to ensure that the models can process the data effectively without being biased by the scale of the original stock prices.\n",
    "\n",
    "3. **Sequence Creation:**\n",
    "    - Sequences for the past 50 days' closing prices (`SEQ_LENGTH = 50`) was generated to serve as input features for the model, with the next days' closing price as the target label.\n",
    "\n",
    "4. **Data Splitting:**\n",
    "    - The data was split into **training**, **validation**, and **test** sets (70%, 15%, and 15%, respectively), ensuring the temporal order was preserved to avoid any data leakage.\n",
    "\n",
    "5. **Saving Preprocessed Data:**\n",
    "    - All preprocessed data (training, validation, test sets) was saved to the `processed/` directory.\n",
    "    - The fitted `MinMaxScaler` was also saved to ensure consistent scaling is applied when making predictions or reversing the scaling.\n",
    "\n",
    "This phase sets the foundation for building, training, and evaluating RNN, LSTM, and GRU models in the upcoming steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
